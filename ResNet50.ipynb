{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_FP51wh7fX6"
      },
      "source": [
        "0) IMPORT ทุกอย่างที่ต้องใช้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6WLiLMa7fX8",
        "outputId": "ec4ffeb3-31b0-4f68-8a9f-cee77cc59067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import kagglehub   # ใช้โหลด dataset จาก Kaggle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ใช้ GPU ถ้ามี ไม่มีก็ใช้ CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsx2ZyuG7fX-"
      },
      "source": [
        "2. Load Dataset (Kaggle BUSI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyufcMa67fX_",
        "outputId": "8111caf1-1c9d-4eda-c862-b8949f727876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/aryashah2k/breast-ultrasound-images-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195M/195M [00:01<00:00, 161MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset path: /root/.cache/kagglehub/datasets/aryashah2k/breast-ultrasound-images-dataset/versions/1\n",
            "Classes: ['benign', 'normal', 'malignant']\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# โหลด dataset\n",
        "dpath = kagglehub.dataset_download(\"aryashah2k/breast-ultrasound-images-dataset\")\n",
        "print(\"Dataset path:\", dpath)\n",
        "\n",
        "folder = os.path.join(dpath, \"Dataset_BUSI_with_GT\")\n",
        "print(\"Classes:\", os.listdir(folder))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk1kSzZe7fX_"
      },
      "source": [
        "3. Build DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GICxe41V7fX_",
        "outputId": "f1df78b8-3090-4a88-fc58-af356aebfc54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 1578\n"
          ]
        }
      ],
      "source": [
        "class_names = [\"benign\", \"malignant\", \"normal\"]\n",
        "data = []\n",
        "\n",
        "for idx, cls in enumerate(class_names):\n",
        "    cdir = os.path.join(folder, cls)\n",
        "    for fname in os.listdir(cdir):\n",
        "        if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            data.append([os.path.join(cdir, fname), idx])\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"path\",\"label\"])\n",
        "print(\"Total images:\", len(df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h0RtNvt7fX_"
      },
      "source": [
        "4. Train/Val/Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8RyUloU7fX_",
        "outputId": "b93e8a9a-60fb-4615-d91c-128ca1110762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1104 237 237\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.15, stratify=df[\"label\"], random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1765, stratify=train_df[\"label\"], random_state=42)\n",
        "\n",
        "print(len(train_df), len(val_df), len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Siguwqgd7fYA"
      },
      "source": [
        "5. Dataset Class + CLAHE + Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lJNVH65C7fYA"
      },
      "outputs": [],
      "source": [
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.15, contrast=0.15),\n",
        "    transforms.Normalize([0.485]*3, [0.229]*3),\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485]*3, [0.229]*3),\n",
        "])\n",
        "\n",
        "\n",
        "class BUSIDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.loc[idx, \"path\"]\n",
        "        label = self.df.loc[idx, \"label\"]\n",
        "\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "        img = clahe.apply(img)\n",
        "        img = cv2.resize(img, (224,224))\n",
        "\n",
        "        img = np.stack([img, img, img], axis=-1)\n",
        "        img = img.astype(\"uint8\")\n",
        "\n",
        "        img = transforms.ToPILImage()(img)\n",
        "        img = self.transform(img)\n",
        "\n",
        "        return {\"image\": img, \"label\": torch.tensor(label).long()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NYLhsZW7fYA"
      },
      "source": [
        "6. DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qmCfwKEO7fYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8054556a-2b38-4d10-d530-a805edebd730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts: [623 295 186]\n",
            "Class weights: [0.00160514 0.00440678 0.00537634]\n"
          ]
        }
      ],
      "source": [
        "# 6. DataLoader + WeightedRandomSampler\n",
        "\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "train_ds = BUSIDataset(train_df, train_tf)\n",
        "val_ds   = BUSIDataset(val_df,   val_tf)\n",
        "test_ds  = BUSIDataset(test_df,  val_tf)\n",
        "\n",
        "# ---- นับจำนวนตัวอย่างแต่ละคลาสใน train_df ----\n",
        "class_counts = train_df[\"label\"].value_counts().sort_index().to_numpy()\n",
        "\n",
        "# base weights = inverse frequency\n",
        "class_weights = 1.0 / class_counts\n",
        "\n",
        "# ⭐ เพิ่มน้ำหนักให้ malignant (class 1) เพื่อดัน recall\n",
        "class_weights[1] *= 1.3   # ลอง 2.0 ก่อน ถ้าอยากแรงขึ้นค่อยขยับ 2.5 / 3.0 ได้\n",
        "\n",
        "print(\"Class counts:\", class_counts)\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# แปลง label ของแต่ละตัวอย่าง -> weight\n",
        "sample_weights = train_df[\"label\"].map(lambda y: class_weights[y]).to_numpy()\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=torch.DoubleTensor(sample_weights),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ❗ เมื่อใช้ sampler ไม่ต้อง shuffle แล้ว\n",
        "train_loader = DataLoader(train_ds, batch_size=32, sampler=sampler)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
        "dataset_sizes = {\"train\": len(train_ds), \"val\": len(val_ds)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmNmaHmC7fYA"
      },
      "source": [
        "7. Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q0mSnl477fYB"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce = nn.CrossEntropyLoss(reduction=\"none\")(logits, targets)\n",
        "        pt = torch.exp(-ce)\n",
        "        focal = ((1 - pt)**self.gamma) * ce\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha_t = self.alpha.to(logits.device)[targets]\n",
        "            focal = alpha_t * focal\n",
        "\n",
        "        return focal.mean() if self.reduction==\"mean\" else focal.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23cO2Cy87fYB",
        "outputId": "d6f4551c-9e52-40b6-d552-e9d393278d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Focal alpha: tensor([0.5907, 1.8712, 1.9785])\n"
          ]
        }
      ],
      "source": [
        "# compute class weight\n",
        "labels_np = train_df[\"label\"].values\n",
        "class_weights_np = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(labels_np),\n",
        "    y=labels_np\n",
        ")\n",
        "\n",
        "# boost malignant a bit\n",
        "class_weights_np[1] *= 1.5\n",
        "\n",
        "alpha_tensor = torch.tensor(class_weights_np, dtype=torch.float32).to(device)\n",
        "criterion = FocalLoss(alpha=alpha_tensor, gamma=1.5)\n",
        "print(\"Focal alpha:\", alpha_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l302h6E7fYB"
      },
      "source": [
        "8. Hybrid CNN + Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q7L8yuYf7fYB"
      },
      "outputs": [],
      "source": [
        "class CNNTransformerHybrid(nn.Module):\n",
        "    def __init__(self, num_classes=3, backbone=\"resnet18\",\n",
        "                 num_layers=2, nhead=8, dim_feedforward=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        if backbone==\"resnet18\":\n",
        "            resnet = models.resnet18(pretrained=True)\n",
        "            fdim = 512\n",
        "        else:\n",
        "            resnet = models.resnet50(pretrained=True)\n",
        "            fdim = 2048\n",
        "\n",
        "        self.conv1 = resnet.conv1\n",
        "        self.bn1   = resnet.bn1\n",
        "        self.relu  = resnet.relu\n",
        "        self.maxpool = resnet.maxpool\n",
        "        self.layer1 = resnet.layer1\n",
        "        self.layer2 = resnet.layer2\n",
        "        self.layer3 = resnet.layer3\n",
        "        self.layer4 = resnet.layer4\n",
        "\n",
        "        self.fdim = fdim\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,fdim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1,50,fdim))\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=fdim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(fdim),\n",
        "            nn.Linear(fdim, fdim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(fdim//2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x); x=self.layer2(x); x=self.layer3(x); x=self.layer4(x)\n",
        "\n",
        "        B,C,H,W = x.shape\n",
        "        x = x.view(B, C, H*W).transpose(1,2)\n",
        "\n",
        "        cls = self.cls_token.expand(B,-1,-1)\n",
        "        x = torch.cat([cls, x], dim=1)\n",
        "        x = x + self.pos_embed[:,:x.size(1)]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        return self.head(x[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yBqrHe07fYC",
        "outputId": "1b819f40-74b0-485c-d548-a163e9f855d4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 152MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNTransformerHybrid(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (transformer): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): Sequential(\n",
            "    (0): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.3, inplace=False)\n",
            "    (4): Linear(in_features=1024, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model_hybrid = CNNTransformerHybrid(\n",
        "    num_classes=3,\n",
        "    backbone=\"resnet50\"\n",
        ").to(device)\n",
        "\n",
        "print(model_hybrid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9lrLeUK7fYC"
      },
      "source": [
        "9. Two-Phase Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JX7EMBfU7fYC"
      },
      "outputs": [],
      "source": [
        "# -------- CutMix Helpers --------\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[3]; H = size[2]\n",
        "    cut_rat = np.sqrt(1-lam)\n",
        "    cut_w = int(W*cut_rat)\n",
        "    cut_h = int(H*cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    x1 = np.clip(cx-cut_w//2, 0, W)\n",
        "    x2 = np.clip(cx+cut_w//2, 0, W)\n",
        "    y1 = np.clip(cy-cut_h//2, 0, H)\n",
        "    y2 = np.clip(cy+cut_h//2, 0, H)\n",
        "\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    rand_index = torch.randperm(x.size(0)).to(x.device)\n",
        "\n",
        "    y_a = y\n",
        "    y_b = y[rand_index]\n",
        "\n",
        "    x1, y1, x2, y2 = rand_bbox(x.size(), lam)\n",
        "    x[:, :, y1:y2, x1:x2] = x[rand_index, :, y1:y2, x1:x2]\n",
        "\n",
        "    lam = 1 - ((x2-x1)*(y2-y1)/(x.size(-1)*x.size(-2)))\n",
        "    return x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "# -------- Updated Train Function --------\n",
        "def train_model(model, criterion, optimizer, dataloaders, sizes,\n",
        "                num_epochs=10, scheduler=None, phase_name=\"Phase\"):\n",
        "\n",
        "    best_w = None\n",
        "    best_loss = 1e9\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n{phase_name} Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for phase in [\"train\",\"val\"]:\n",
        "            model.train() if phase==\"train\" else model.eval()\n",
        "\n",
        "            running_loss = 0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for batch in dataloaders[phase]:\n",
        "                imgs = batch[\"image\"].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # ---- CutMix (train only) ----\n",
        "                use_cutmix = False\n",
        "                if phase==\"train\" and labels.eq(1).any() and np.random.rand()<0.6:\n",
        "                  use_cutmix = True\n",
        "\n",
        "                if use_cutmix:\n",
        "                    imgs, y_a, y_b, lam = cutmix_data(imgs, labels)\n",
        "                    outputs = model(imgs)\n",
        "                    loss = lam*criterion(outputs, y_a) + (1-lam)*criterion(outputs, y_b)\n",
        "                else:\n",
        "                    outputs = model(imgs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                preds = outputs.argmax(1)\n",
        "\n",
        "                if phase==\"train\":\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()*imgs.size(0)\n",
        "\n",
        "                if use_cutmix:\n",
        "                    running_corrects += lam*(preds==y_a).sum() + (1-lam)*(preds==y_b).sum()\n",
        "                else:\n",
        "                    running_corrects += (preds==labels).sum()\n",
        "\n",
        "            ep_loss = running_loss / sizes[phase]\n",
        "            ep_acc  = running_corrects.double() / sizes[phase]\n",
        "\n",
        "            print(f\"{phase}: loss={ep_loss:.4f} acc={ep_acc:.4f}\")\n",
        "\n",
        "            if phase == \"val\":\n",
        "                if ep_loss < best_loss:\n",
        "                    best_loss = ep_loss\n",
        "                    best_w = model.state_dict()\n",
        "\n",
        "                if scheduler:\n",
        "                    scheduler.step(ep_loss)\n",
        "\n",
        "    model.load_state_dict(best_w)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 1: Freeze CNN backbone\n",
        "for name, param in model_hybrid.named_parameters():\n",
        "    if name.startswith((\"conv1\",\"bn1\",\"layer1\",\"layer2\",\"layer3\",\"layer4\")):\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True\n",
        "\n",
        "optimizer1 = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model_hybrid.parameters()),\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "model_hybrid = train_model(\n",
        "    model_hybrid, criterion, optimizer1,\n",
        "    dataloaders, dataset_sizes,\n",
        "    num_epochs=8,\n",
        "    phase_name=\"Hybrid Phase 1\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z09HA65rUIz7",
        "outputId": "b22df814-c219-4c4a-a4d3-2d7b8cde7772"
      },
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Hybrid Phase 1 Epoch 1/8\n",
            "train: loss=0.9276 acc=0.4196\n",
            "val: loss=1.1569 acc=0.1688\n",
            "\n",
            "Hybrid Phase 1 Epoch 2/8\n",
            "train: loss=0.8621 acc=0.3767\n",
            "val: loss=0.6786 acc=0.2785\n",
            "\n",
            "Hybrid Phase 1 Epoch 3/8\n",
            "train: loss=0.7063 acc=0.4820\n",
            "val: loss=0.6075 acc=0.3249\n",
            "\n",
            "Hybrid Phase 1 Epoch 4/8\n",
            "train: loss=0.6977 acc=0.4806\n",
            "val: loss=0.6696 acc=0.3629\n",
            "\n",
            "Hybrid Phase 1 Epoch 5/8\n",
            "train: loss=0.6221 acc=0.5446\n",
            "val: loss=0.6029 acc=0.3755\n",
            "\n",
            "Hybrid Phase 1 Epoch 6/8\n",
            "train: loss=0.6406 acc=0.5347\n",
            "val: loss=0.5711 acc=0.3671\n",
            "\n",
            "Hybrid Phase 1 Epoch 7/8\n",
            "train: loss=0.6072 acc=0.5587\n",
            "val: loss=0.6003 acc=0.3586\n",
            "\n",
            "Hybrid Phase 1 Epoch 8/8\n",
            "train: loss=0.6845 acc=0.5194\n",
            "val: loss=0.5245 acc=0.3629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qsq-lDf7fYC"
      },
      "source": [
        "Phase 2: Unfreeze layer4 + Transformer + Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbxOeKbx7fYC",
        "outputId": "0041773c-2692-45ab-aa33-474929fd8851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hybrid Phase 2 Epoch 1/12\n",
            "train: loss=0.6594 acc=0.5155\n",
            "val: loss=0.6506 acc=0.3882\n",
            "\n",
            "Hybrid Phase 2 Epoch 2/12\n",
            "train: loss=0.5836 acc=0.5956\n",
            "val: loss=0.5822 acc=0.3797\n",
            "\n",
            "Hybrid Phase 2 Epoch 3/12\n"
          ]
        }
      ],
      "source": [
        "for name, param in model_hybrid.named_parameters():\n",
        "    if name.startswith((\"layer3\",\"layer4\",\"transformer\",\"head\",\"cls_token\",\"pos_embed\")):\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "optimizer2 = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model_hybrid.parameters()),\n",
        "    lr=2e-4,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer2, mode=\"min\", factor=0.5, patience=2\n",
        ")\n",
        "\n",
        "model_hybrid = train_model(\n",
        "    model_hybrid, criterion, optimizer2,\n",
        "    dataloaders, dataset_sizes,\n",
        "    num_epochs=12,\n",
        "    scheduler=scheduler2,\n",
        "    phase_name=\"Hybrid Phase 2\"\n",
        ")\n",
        "\n",
        "best_model_hybrid = model_hybrid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTQIeTka7fYC"
      },
      "source": [
        "10. Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKhK11Wk7fYC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "best_model_hybrid.eval()\n",
        "probs=[]\n",
        "labels=[]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        x=batch[\"image\"].to(device)\n",
        "        y=batch[\"label\"].to(device)\n",
        "        o=best_model_hybrid(x)\n",
        "        p=torch.softmax(o,1)\n",
        "\n",
        "        probs.append(p.cpu().numpy())\n",
        "        labels.append(y.cpu().numpy())\n",
        "\n",
        "y_pred_proba=np.concatenate(probs)\n",
        "y_test=np.concatenate(labels)\n",
        "y_pred=np.argmax(y_pred_proba,1)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Macro F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
        "print(\"Macro Recall:\", recall_score(y_test, y_pred, average=\"macro\"))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix - Hybrid CNN+Transformer\")\n",
        "plt.colorbar()\n",
        "\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names, rotation=45)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "\n",
        "thresh = cm.max() / 2.\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel(\"True label\")\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UFGZMNdISoSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXZZOnLP7fYC"
      },
      "source": [
        "11. Threshold tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jREjkIhW7fYD"
      },
      "outputs": [],
      "source": [
        "def eval_threshold(y_pred_proba, y_true, th):\n",
        "    yp=[]\n",
        "    for p in y_pred_proba:\n",
        "        if p[1] >= th:\n",
        "            yp.append(1)\n",
        "        else:\n",
        "            yp.append(0 if p[0]>=p[2] else 2)\n",
        "\n",
        "    print(\"\\n=== T =\",th,\"===\")\n",
        "    print(\"Acc:\", accuracy_score(y_true, yp))\n",
        "    print(\"Macro F1:\", f1_score(y_true, yp, average=\"macro\"))\n",
        "    print(\"Macro Recall:\", recall_score(y_true, yp, average=\"macro\"))\n",
        "    print(confusion_matrix(y_true, yp))\n",
        "\n",
        "for th in [0.30,0.35,0.40,0.45,0.50]:\n",
        "    eval_threshold(y_pred_proba, y_test, th)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grad-CAM"
      ],
      "metadata": {
        "id": "j4NWoV5QJH3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# transform แบบไม่มี augmentation ใช้สำหรับ Grad-CAM\n",
        "cam_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485]*3, [0.229]*3),\n",
        "])\n",
        "IMG_SIZE = 224"
      ],
      "metadata": {
        "id": "5ooWVw1xxPIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        \"\"\"\n",
        "        model: โมเดล PyTorch\n",
        "        target_layer: เลเยอร์ convolution ที่อยากดู เช่น model.layer4\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "\n",
        "        # forward hook: เก็บ feature map\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()\n",
        "\n",
        "        # backward hook: เก็บ gradient ของ feature map\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            # grad_out[0] คือ gradient w.r.t. output\n",
        "            self.gradients = grad_out[0].detach()\n",
        "\n",
        "        self.fh = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.bh = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def generate(self, x, class_idx=None):\n",
        "        \"\"\"\n",
        "        x: tensor (1,3,H,W)\n",
        "        class_idx: index ของคลาสที่อยากทำ Grad-CAM (0/1/2)\n",
        "                   ถ้า None จะใช้ predicted class\n",
        "        return: heatmap (numpy 2D, ขนาด HxW, normalized 0-1)\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        x = x.to(device)\n",
        "        scores = self.model(x)  # (1, num_classes)\n",
        "\n",
        "        if class_idx is None:\n",
        "            class_idx = scores.argmax(dim=1).item()\n",
        "\n",
        "        target = scores[0, class_idx]\n",
        "        target.backward()\n",
        "\n",
        "        # activations: (B,C,H,W)\n",
        "        # gradients:   (B,C,H,W)\n",
        "        acts = self.activations[0]   # (C,H,W)\n",
        "        grads = self.gradients[0]    # (C,H,W)\n",
        "\n",
        "        # global average pooling บน gradient\n",
        "        weights = grads.mean(dim=(1,2))   # (C,)\n",
        "\n",
        "        # weighted sum ของ feature map\n",
        "        cam = torch.zeros_like(acts[0])\n",
        "        for w, a in zip(weights, acts):\n",
        "            cam += w * a\n",
        "\n",
        "        cam = torch.relu(cam)\n",
        "        cam -= cam.min()\n",
        "        cam /= (cam.max() + 1e-8)\n",
        "\n",
        "        return cam.cpu().numpy()  # (H,W)\n",
        "\n",
        "    def close(self):\n",
        "        self.fh.remove()\n",
        "        self.bh.remove()"
      ],
      "metadata": {
        "id": "zJrs9OmEGAJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradcam = GradCAM(best_model_hybrid, best_model_hybrid.layer4)"
      ],
      "metadata": {
        "id": "mpPTSWcyGHNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def show_gradcam_on_image(img_path, true_label=None, class_idx=None):\n",
        "    \"\"\"\n",
        "    img_path: path รูปใน test set\n",
        "    true_label: label จริง (0/1/2) ไว้โชว์เฉย ๆ\n",
        "    class_idx: class ที่จะทำ Grad-CAM (ถ้า None = predicted class)\n",
        "    \"\"\"\n",
        "    # ----- โหลดภาพต้นฉบับแบบ grayscale -----\n",
        "    orig = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    h0, w0 = orig.shape[:2]\n",
        "\n",
        "    # ทำ CLAHE ให้เหมือนตอนเทรน\n",
        "    img_clahe = clahe.apply(orig)\n",
        "    img_resized = cv2.resize(img_clahe, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # ทำเป็น 3-channel\n",
        "    img_3ch = np.stack([img_resized]*3, axis=-1).astype(\"uint8\")\n",
        "\n",
        "    # แปลงเป็น tensor และ normalize\n",
        "    pil_img = transforms.ToPILImage()(img_3ch)\n",
        "    x = cam_tf(pil_img).unsqueeze(0)  # (1,3,224,224)\n",
        "\n",
        "    # ----- ทำ Grad-CAM -----\n",
        "    cam = gradcam.generate(x, class_idx=class_idx)  # (Hc,Wc) เช่น 7x7 -> ถูก upsample ในขั้นต่อไป\n",
        "\n",
        "    # resize CAM ให้เท่ากับภาพต้นฉบับ\n",
        "    cam_resized = cv2.resize(cam, (w0, h0))\n",
        "\n",
        "    # ทำเป็น heatmap สี\n",
        "    heatmap = np.uint8(255 * cam_resized)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    # รวม heatmap กับภาพ grayscale เดิม (ต้องแปลงเป็น BGR 3 ช่อง)\n",
        "    orig_bgr = cv2.cvtColor(orig, cv2.COLOR_GRAY2BGR)\n",
        "    overlay = cv2.addWeighted(orig_bgr, 0.5, heatmap, 0.5, 0)\n",
        "\n",
        "    # แปลงเป็น RGB สำหรับ matplotlib\n",
        "    orig_rgb = cv2.cvtColor(orig_bgr, cv2.COLOR_BGR2RGB)\n",
        "    overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # ----- ทำนาย class ด้วยโมเดล -----\n",
        "    best_model_hybrid.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = best_model_hybrid(x.to(device))\n",
        "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "        pred_class = np.argmax(probs)\n",
        "\n",
        "    # ----- แสดงผล -----\n",
        "    plt.figure(figsize=(10,4))\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.imshow(orig_rgb, cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Original\\nlabel={true_label}\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.imshow(cam_resized, cmap=\"jet\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Grad-CAM (mask)\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.imshow(overlay_rgb)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Overlay\\npred={pred_class}, probs={probs.round(2)}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VSC2LPXzGLuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# เลือก index ของ malignant จาก test_df\n",
        "mal_idx = test_df[test_df[\"label\"] == 1].index[0]\n",
        "mal_path = test_df.loc[mal_idx, \"path\"]\n",
        "mal_label = test_df.loc[mal_idx, \"label\"]\n",
        "\n",
        "print(\"Path:\", mal_path, \"Label:\", mal_label)  # label=1 คือ malignant\n",
        "\n",
        "show_gradcam_on_image(mal_path, true_label=mal_label)\n"
      ],
      "metadata": {
        "id": "sOyQNnSqGP7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# benign\n",
        "ben_idx = test_df[test_df[\"label\"] == 0].index[0]\n",
        "ben_path = test_df.loc[ben_idx, \"path\"]\n",
        "ben_label = test_df.loc[ben_idx, \"label\"]\n",
        "show_gradcam_on_image(ben_path, true_label=ben_label)\n",
        "\n",
        "# normal\n",
        "nor_idx = test_df[test_df[\"label\"] == 2].index[0]\n",
        "nor_path = test_df.loc[nor_idx, \"path\"]\n",
        "nor_label = test_df.loc[nor_idx, \"label\"]\n",
        "show_gradcam_on_image(nor_path, true_label=nor_label)\n"
      ],
      "metadata": {
        "id": "VBG_wiJoGTCp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iTVBhzYanZ65",
        "DGOcjhJGnZ66",
        "G8nvJBlSnZ66",
        "llpL4cSynZ67",
        "U1ArlSy0xDuI",
        "KMkw3IehxV4m",
        "_nm3m3n7xz5M"
      ],
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 1209633,
          "sourceId": 2021025,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}